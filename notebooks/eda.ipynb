{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will try to predict the upvote score of posts on the Hacker News website https://news.ycombinator.com/ using just their titles.\n",
    "\n",
    "From this week onward, you are allowed (and recommneded) to use PyTorch and its libraries.\n",
    "\n",
    "This is the recipe which we suggest.\n",
    "\n",
    "1. Import SentencePiece to use for tokenizing our data.\n",
    "2. Prepare the dataset of Hacker News titles and upvote scores\n",
    "    - Obtain the data from the database [DATABASE_LINK]\n",
    "    - Tokenise the titles using SentencePiece\n",
    "3. Implement and train an architecture to obtain word embeddings in the style of the word2vec paper\n",
    "https://arxiv.org/pdf/1301.3781.pdf\n",
    "using either the *continuous bag of words (CBOW) or Skip-gram model (or both).\n",
    "4. Implement a regression model to predict a Hacker News upvote score from the pooled average of the word embeddings in each title.\n",
    "5. Extension: train your word embeddings on a different dataset, such as\n",
    "- More Hacker News content, such as comments\n",
    "- A completely different corpus of text, like (some of) Wikipedia\n",
    "\n",
    "In the following chapters of this week's Cortex module, we will delve into some of these steps and the technologies which they use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia \n",
    "6GB worth of compressed articles. Which you can load as follows:\n",
    "```\n",
    "import datasets\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20200501.en\")\n",
    "```\n",
    "\n",
    "There is also a summary dataset: https://github.com/tscheepers/Wikipedia-Summary-Dataset\n",
    "which is 5.1 Million wiki articles which is 2.7Gb uncompressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the wiki dataset using gensim downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# https://github.com/piskvorky/gensim-data/releases/tag/wiki-english-20171001\n",
    "dataset = api.load(\"wiki-english-20171001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4924894\n"
     ]
    }
   ],
   "source": [
    "print(len([1 for i in dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the large 6GB wikipedia dump\n",
    "UPDATE: This has lots of wiki markup in it. So not useful for our purposes. We will use the summary dataset instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=69962\n",
      "\t\t'''Anarchism''' is a political philosophy that advocates self-governed societies based on voluntary \n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=184\n",
      "\t\tIntroduction Etymology and terminology History Anarchist schools of thought Internal issues and deba\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "1th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=54309\n",
      "\t\t'''Autism''' is a neurodevelopmental disorder characterized by impaired social interaction, impaired\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=175\n",
      "\t\tIntroduction Characteristics Causes Mechanism Diagnosis Screening Prevention Management Society and \n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "2th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=18444\n",
      "\t\tPercentage of diffusely reflected sunlight in relation to various surface conditions\n",
      "\n",
      "'''Albedo''' (\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=149\n",
      "\t\tIntroduction Terrestrial albedo  Astronomical albedo  Examples of terrestrial albedo effects Other t\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "3th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=10691\n",
      "\t\tWriting cursive forms of A\n",
      "'''A''' (named , plural ''As'', ''A's'', ''a''s, ''a's'' or ''aes'') is t\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=153\n",
      "\t\tIntroduction History Use in writing systems Other uses Related characters Computing codes Other repr\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "4th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=84366\n",
      "\t\t'''Alabama''' () is a state in the southeastern region of the United States. It is bordered by Tenne\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=170\n",
      "\t\tIntroduction Etymology History Geography Demographics Economy Law and government Education Media Cul\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "5th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=43399\n",
      "\t\tAchilles and the Nereid Cymothoe, Attic red-figure kantharos from Volci (Cabinet des Médailles, Bibl\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=236\n",
      "\t\tIntroduction  Etymology   Birth and early years   Achilles in the Trojan War   Worship and heroic cu\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "6th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=102633\n",
      "\t\t'''Abraham Lincoln''' (; February 12, 1809 – April 15, 1865) was an American politician and lawyer w\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=319\n",
      "\t\tIntroduction Family and childhood Early career and militia service U.S. House of Representatives, 18\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "7th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=73158\n",
      "\t\t'''Aristotle''' (; , , ''Aristotélēs''; 384–322 BC) was an ancient Greek philosopher and scientist b\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=152\n",
      "\t\tIntroduction  Life  Thought Loss and preservation of his works Legacy List of works Eponyms See also\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "8th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=11690\n",
      "\t\t'''''An American in Paris''''' is a jazz-influenced orchestral piece by the American composer George\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=146\n",
      "\t\tIntroduction Background Composition Instrumentation Response Preservation status Recordings Use in f\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "9th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=41797\n",
      "\t\tThe '''Academy Award for Best Production Design''' recognizes achievement for art direction in film.\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=112\n",
      "\t\tIntroduction Superlatives Best Interior Decoration Best Art Direction &ndash; Set Decoration See als\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "10th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=35718\n",
      "\t\tThe '''Academy Awards''', now known officially as '''The Oscars''', is a set of twenty-four awards f\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=281\n",
      "\t\tIntroduction History Oscar statuette Nomination Awards ceremonies Venues Awards of Merit categories \n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n",
      "11th article\n",
      "\tsection_texts; type(val)=<class 'list'> len=2407\n",
      "\t\t'''''Actresses''''' (Catalan: '''''Actrius''''') is a 1997 Catalan language Spanish drama film produ\n",
      "\n",
      "\tsection_titles; type(val)=<class 'list'> len=64\n",
      "\t\tIntroduction Synopsis Cast Recognition References External links\n",
      "\n",
      "\ttitle; type(val)=<class 'str'> \n"
     ]
    }
   ],
   "source": [
    "# def load_n_articles(dataset, n=100)\n",
    "for i, thing in enumerate(dataset):\n",
    "    # print(i, thing, type(thing))\n",
    "    print(f\"{i}th article\")\n",
    "    for k, v in thing.items():\n",
    "        print(f\"\\t{k}; type(val)={type(v)}\", end=\" \")\n",
    "        if isinstance(v, str) == str or isinstance(v, list):\n",
    "            if isinstance(v, list) and isinstance(v[0], str):\n",
    "                string_text = \" \".join(v).strip()\n",
    "                print(f\"len={len(string_text)}\")\n",
    "                print(f\"\\t\\t{string_text[0:100]}\")\n",
    "            else:\n",
    "                print(f\"len={len(v)}\")\n",
    "                print(f\"\\t{k}: {v[0:100]}\")\n",
    "        print()\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_summary_articles(summary_file_path=..., start=0, end=100):\n",
    "    \"\"\"\n",
    "    read start to end lines from data/dir within the summary_file_path without reading in the whole\n",
    "    file in python\n",
    "    \"\"\"\n",
    "    summary_dataset = []\n",
    "    for i in wiki_article_iterator(summary_file_path, start, end):\n",
    "        summary_dataset.append(i)\n",
    "    return summary_dataset\n",
    "    \n",
    "def wiki_article_iterator(summary_file_path=..., start=0, end=100):\n",
    "    \"\"\"\n",
    "    read start to end lines from data/dir within the summary_file_path without reading in the whole\n",
    "    file in python\n",
    "    \"\"\"\n",
    "    with open(summary_file_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i > end:\n",
    "                break\n",
    "            if \"|||\" in line:\n",
    "                yield line.split(\"|||\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/recurrent_rebels_week2_hackernews_upvotes/notebooks/data/raw.txt True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# make the current project path + summary_file_path\n",
    "summary_file_path = \"data/raw.txt\"\n",
    "# full_path = os.path.join(os.getcwd(), summary_file_path)\n",
    "full_path = Path.cwd() / summary_file_path\n",
    "print(full_path, full_path.exists())\n",
    "assert full_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism is a political philosophy that advocates self-governed societies based on voluntary institutions. These are often described as stateless societies, although several authors have defined them more specifically as institutions based on non-hierarchical free associations. Anarchism holds the state to be undesirable, unnecessary and harmful. While anti-statism is central, anarchism specifically entails opposing authority or hierarchical organisation in the conduct of all human relations, including—but not limited to—the state system. Anarchism is usually considered an extreme left-wing ideology and much of anarchist economics and anarchist legal philosophy reflects anti-authoritarian interpretations of communism, collectivism, syndicalism, mutualism or participatory economics. Anarchism does not offer a fixed body of doctrine from a single particular world view, instead fluxing and flowing as a philosophy. Many types and traditions of anarchism exist, not all of which are mutually exclusive. Anarchist schools of thought can differ fundamentally, supporting anything from extreme individualism to complete collectivism. Strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications.\n",
      "5315384\n",
      "total of 2706383741 characters in 5315384 articles\n"
     ]
    }
   ],
   "source": [
    "articles = load_n_summary_articles(full_path, 0, 1e9)\n",
    "print(articles[0])\n",
    "print(len(articles))\n",
    "print(\n",
    "    f\"total of {sum([len(a) for a in articles])} characters in {len(articles)} articles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Train in module sentencepiece:\n",
      "\n",
      "Train(arg=None, logstream=None, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the sentence piece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "import sentencepiece as spm\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ARTICLES = 5315384\n",
    "model = io.BytesIO()\n",
    "# for article in articles[0:100]:\n",
    "iterator = wiki_article_iterator(full_path, 0, 5e6)\n",
    "spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=model, vocab_size=VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexandra Hills is a locality in Redland City, Queensland, Australia. Alexandra Hills sits between two major areas of Redlands, with Cleveland to the east and Capalaba to the west.\n",
      "▁Alexandr a ▁Hills ▁is ▁a ▁local ity ▁in ▁Red land ▁City , ▁Queensland , ▁Australia . ▁Alexandr a ▁Hills ▁si ts ▁between ▁two ▁major ▁areas ▁of ▁Red lands , ▁with ▁Cleveland ▁to ▁the ▁east ▁and ▁Cap al aba ▁to ▁the ▁west .\n",
      "[7560, 45, 3821, 12, 10, 456, 261, 9, 1080, 289, 303, 4, 4506, 4, 507, 6, 7560, 45, 3821, 2717, 684, 131, 85, 447, 1052, 7, 1080, 3420, 4, 26, 4914, 11, 3, 581, 8, 4166, 53, 5209, 11, 3, 593, 6]\n"
     ]
    }
   ],
   "source": [
    "print(articles[1000001])\n",
    "print(' '.join(sp_processor.encode_as_pieces(articles[1000001])))\n",
    "print(sp_processor.encode_as_ids(articles[1000001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model_path = Path.cwd() / \"model/spm_1e6.model\"\n",
    "Path.mkdir(sp_model_path.parent, exist_ok=True)\n",
    "\n",
    "with open(sp_model_path, \"wb\") as f:\n",
    "    f.write(model.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_processor = spm.SentencePieceProcessor(model_proto=model.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.utils.SaveLoad)\n",
      " |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
      " |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |      \n",
      " |      Once you're finished training a model (=no more updates, only querying)\n",
      " |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      " |      to reduce memory.\n",
      " |      \n",
      " |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |      \n",
      " |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, hierarchical softmax will not be used for model training.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If 0, negative sampling will not be used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, rethrow=False, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sp_processor.encode_as_pieces(articles[:int(1e6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the word2vec model to get the word embeddings for the tokens from sentence piece\n",
    "# Trained on the same dataset of 1e6 articles used for the sentence piece model.\n",
    "from gensim.models import Word2Vec\n",
    "reference_w2v_model = Word2Vec(\n",
    "         sentences=X, \n",
    "         vector_size=1024, \n",
    "         window=8, # Maximum distance between the current and predicted word within a sentence.\n",
    "         min_count=10,  # Minimum frequency below which a token is ignored\n",
    "         workers=4, \n",
    "         sg=1, # Skip gram\n",
    "         epochs=10,\n",
    "         max_vocab_size=VOCAB_SIZE,\n",
    "         compute_loss=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reference_w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreference_w2v_model\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reference_w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "reference_w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adhoc testing of the word2Vec model\n",
    "Write a The king - man + woman = queen example test with the trained embeddings.\n",
    "Another is to plot the loss as its trained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     sp\u001b[38;5;241m.\u001b[39mLoad(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwiki-ml-100k.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sp\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sentencepiece_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwiki-ml-100k.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Quick eval:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m## Generate example embeddings\u001b[39;00m\n\u001b[1;32m     20\u001b[0m test_words \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhiking\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m, in \u001b[0;36mcreate_sentencepiece_model\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_sentencepiece_model\u001b[39m(data):\n\u001b[0;32m---> 12\u001b[0m     sp \u001b[38;5;241m=\u001b[39m \u001b[43mspm\u001b[49m\u001b[38;5;241m.\u001b[39mSentencePieceProcessor()\n\u001b[1;32m     13\u001b[0m     sp\u001b[38;5;241m.\u001b[39mLoad(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwiki-ml-100k.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sp\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spm' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize wikipedia `articles` list using sentence piece.\n",
    "# Feed the tokens to gensim and train word2vec model.\n",
    "# Save the model to disk.\n",
    "\n",
    "\n",
    "\n",
    "model = create_sentencepiece_model(\"wiki-ml-100k.txt\")\n",
    "# Quick eval:\n",
    "## Generate example embeddings\n",
    "test_words = [\n",
    "    \"queen\",\n",
    "    \"woman\",\n",
    "    \"mother\",\n",
    "    \"wife\",\n",
    "    \"new delhi\",\n",
    "    \"mughals\",\n",
    "    \"taj mahal\",\n",
    "    \"hotel\",\n",
    "    \"vacation\",\n",
    "    \"hiking\",\n",
    "]\n",
    "## Visualize a set of vectors using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m db_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres://arcanum:nz2TBHLHl8VSBTSxznk@pg.mlx.institute:5433/arcanum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create the database engine\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_engine\u001b[49m(db_url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace these values with your PostgreSQL credentials\n",
    "DATABASE = {\n",
    "    \"drivername\": \"postgresql\",\n",
    "    \"host\": \"your_host\",\n",
    "    \"port\": \"your_port\",\n",
    "    \"username\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"database\": \"your_database\",\n",
    "}\n",
    "\n",
    "# Format the database URL\n",
    "# db_url = f\"{DATABASE['drivername']}://{DATABASE['username']}:{DATABASE['password']}@{DATABASE['host']}:{DATABASE['port']}/{DATABASE['database']}\"\n",
    "db_url = \"postgres://arcanum:nz2TBHLHl8VSBTSxznk@pg.mlx.institute:5433/arcanum\"\n",
    "\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
